# train.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import os

from unet import UNet
from dataset import DenoisingDataset

# è‡ªå®šä¹‰åŠ æƒ L1 æŸå¤±å‡½æ•°
def weighted_l1_loss(pred, target, foreground_weight=10.0, background_weight=1.0):
    """
    pred: ç½‘ç»œè¾“å‡º [B, 3, H, W]ï¼Œå€¼èŒƒå›´ [0, 1]
    target: çœŸå® clean å›¾ [B, 3, H, W]ï¼Œå€¼èŒƒå›´ [0, 1]
    foreground_weight: æ–‡å­—ï¼ˆæš—åŒºï¼‰æƒé‡
    background_weight: èƒŒæ™¯ï¼ˆäº®åŒºï¼‰æƒé‡
    """
    # è®¡ç®—ç»å¯¹è¯¯å·®
    l1_loss = torch.abs(pred - target)  # [B, 3, H, W]

    # åˆ›å»ºæƒé‡çŸ©é˜µï¼šæ–‡å­—åŒºåŸŸæƒé‡é«˜ï¼ŒèƒŒæ™¯æƒé‡ä½
    weights = torch.where(
        target < 0.2,           # å¦‚æœ target åƒç´  < 0.2 â†’ å¾ˆå¯èƒ½æ˜¯æ–‡å­—ï¼ˆé»‘ï¼‰
        foreground_weight,      # é«˜æƒé‡
        background_weight       # å¦åˆ™æ˜¯èƒŒæ™¯ â†’ ä½æƒé‡
    )

    # åŠ æƒæŸå¤±
    weighted_loss = l1_loss * weights
    return weighted_loss.mean()

# å‚æ•°è®¾ç½®
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
EPOCHS = 40
BATCH_SIZE = 4  # æ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼ˆRTX 3090 å¯ç”¨ 8ï¼‰
LR = 1e-4
IMG_SIZE = 512  # ç»Ÿä¸€åˆ†è¾¨ç‡

noisy_dir = "./data/noisy"
clean_dir = "./data/clean"
model_save_path = "./checkpoints/unet_denoise_rgb.pth"

# æ•°æ®é›†
dataset = DenoisingDataset(noisy_dir, clean_dir, image_size=IMG_SIZE)
dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)

print(f"   å›¾åƒæ•°é‡: {len(dataset)}")
if len(dataset) == 0:
    raise ValueError("âŒ æ•°æ®é›†ä¸ºç©ºï¼è¯·æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦æœ‰å›¾ç‰‡æ–‡ä»¶")

# æ¨¡å‹
model = UNet(in_channels=3, out_channels=3).to(DEVICE)
optimizer = optim.Adam(model.parameters(), lr=LR)
criterion = nn.L1Loss()  # æ¨èç”¨äºå›¾åƒé‡å»º

# è®­ç»ƒ
model.train()
for epoch in range(EPOCHS):
    epoch_loss = 0.0
    with tqdm(dataloader, unit="batch") as tepoch:
        tepoch.set_description(f"Epoch {epoch+1}/{EPOCHS}")

        for noisy_batch, clean_batch in tepoch:
            noisy_batch = noisy_batch.to(DEVICE)  # [B, 3, H, W]
            clean_batch = clean_batch.to(DEVICE)

            optimizer.zero_grad()
            outputs = model(noisy_batch)

            loss = weighted_l1_loss(outputs, clean_batch, foreground_weight=10.0, background_weight=1.0)

            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            tepoch.set_postfix(loss=loss.item())

            # è°ƒè¯•ï¼šæ£€æŸ¥è¾“å‡ºèŒƒå›´
            if epoch == 0 and loss.item() < 1e-5:
                print("\nâš ï¸  æŸå¤±è¿‡å°ï¼Œå¯èƒ½æ•°æ®ç›¸åŒæˆ–åŠ è½½é”™è¯¯")
            if torch.isnan(loss):
                print("\nğŸ’¥ æŸå¤±ä¸º NaNï¼æ¢¯åº¦çˆ†ç‚¸")
                break

    avg_loss = epoch_loss / len(dataloader)
    print(f"Epoch [{epoch+1}/{EPOCHS}], Average Loss: {avg_loss:.6f}")

    # æ¯10è½®ä¿å­˜ä¸€æ¬¡
    if (epoch + 1) % 10 == 0:
        torch.save(model.state_dict(), f"./checkpoints/unet_epoch_{epoch+1}.pth")

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
torch.save(model.state_dict(), model_save_path)
print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜: {model_save_path}")
